{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ† SOCAR Hackathon 2025 - Handwriting OCR Training\n",
    "\n",
    "**Complete implementation of Hybrid OCR Architecture on real handwriting data**\n",
    "\n",
    "This notebook:\n",
    "1. Downloads real handwriting dataset from Kaggle\n",
    "2. Implements preprocessing pipeline\n",
    "3. Fine-tunes TrOCR model\n",
    "4. Implements ensemble approach\n",
    "5. Evaluates performance\n",
    "6. Visualizes results\n",
    "\n",
    "**Dataset**: Handwritten2Text Training Dataset (Kaggle)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Dataset Download](#dataset)\n",
    "3. [Data Exploration](#exploration)\n",
    "4. [Preprocessing Pipeline](#preprocessing)\n",
    "5. [Model Training](#training)\n",
    "6. [Ensemble Implementation](#ensemble)\n",
    "7. [Evaluation](#evaluation)\n",
    "8. [Demo & Visualization](#demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Installation ðŸ”§\n",
    "\n",
    "Install all required packages for our hybrid OCR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"âœ… Running in Google Colab\")\n",
    "    # Check GPU\n",
    "    !nvidia-smi -L\n",
    "else:\n",
    "    print(\"âš ï¸ Not in Colab - some features may not work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages (suppress output)\n",
    "!pip install -q kagglehub\n",
    "!pip install -q transformers>=4.35.0\n",
    "!pip install -q datasets\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q Pillow\n",
    "!pip install -q opencv-python\n",
    "!pip install -q albumentations\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q jiwer  # For CER/WER calculation\n",
    "!pip install -q tqdm\n",
    "!pip install -q accelerate\n",
    "\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from jiwer import cer, wer\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸŽ® CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Dataset Download ðŸ“¥\n",
    "\n",
    "Download the handwriting dataset from Kaggle using kagglehub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "print(\"ðŸ“¥ Downloading dataset from Kaggle...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "path = kagglehub.dataset_download(\"chaimaourgani/handwritten2text-training-dataset\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset downloaded!\")\n",
    "print(f\"ðŸ“ Path to dataset files: {path}\")\n",
    "\n",
    "# List files\n",
    "dataset_path = Path(path)\n",
    "all_files = list(dataset_path.rglob('*'))\n",
    "print(f\"\\nðŸ“Š Total files: {len(all_files)}\")\n",
    "print(\"\\nðŸ“‚ Directory structure:\")\n",
    "for item in list(dataset_path.glob('*'))[:10]:\n",
    "    print(f\"  - {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Exploration ðŸ”\n",
    "\n",
    "Let's explore the dataset structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image and text files\n",
    "image_files = list(dataset_path.rglob('*.jpg')) + list(dataset_path.rglob('*.png'))\n",
    "text_files = list(dataset_path.rglob('*.txt'))\n",
    "\n",
    "print(f\"ðŸ“· Image files: {len(image_files)}\")\n",
    "print(f\"ðŸ“ Text files: {len(text_files)}\")\n",
    "\n",
    "# Show some examples\n",
    "if image_files:\n",
    "    print(f\"\\nðŸ“· Sample image files:\")\n",
    "    for img in image_files[:5]:\n",
    "        print(f\"  - {img.name}\")\n",
    "\n",
    "if text_files:\n",
    "    print(f\"\\nðŸ“ Sample text files:\")\n",
    "    for txt in text_files[:5]:\n",
    "        print(f\"  - {txt.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset mapping\n",
    "def find_matching_pairs(dataset_path):\n",
    "    \"\"\"Find matching image-text pairs.\"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    # Look for common patterns\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    \n",
    "    for img_path in dataset_path.rglob('*'):\n",
    "        if img_path.suffix.lower() in image_extensions:\n",
    "            # Try to find corresponding text file\n",
    "            txt_path = img_path.with_suffix('.txt')\n",
    "            if txt_path.exists():\n",
    "                pairs.append({\n",
    "                    'image': str(img_path),\n",
    "                    'text_file': str(txt_path)\n",
    "                })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Find pairs\n",
    "data_pairs = find_matching_pairs(dataset_path)\n",
    "print(f\"âœ… Found {len(data_pairs)} image-text pairs\")\n",
    "\n",
    "# If no pairs found, try alternative method\n",
    "if len(data_pairs) == 0:\n",
    "    print(\"\\nâš ï¸ No direct pairs found, trying alternative structure...\")\n",
    "    \n",
    "    # Check for CSV or JSON metadata\n",
    "    csv_files = list(dataset_path.rglob('*.csv'))\n",
    "    json_files = list(dataset_path.rglob('*.json'))\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"\\nðŸ“Š Found CSV files: {[f.name for f in csv_files]}\")\n",
    "        # Try to load first CSV\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        print(f\"\\nCSV columns: {df.columns.tolist()}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "    \n",
    "    if json_files:\n",
    "        print(f\"\\nðŸ“„ Found JSON files: {[f.name for f in json_files]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "def visualize_samples(data_pairs, n_samples=6):\n",
    "    \"\"\"Visualize random samples from dataset.\"\"\"\n",
    "    if len(data_pairs) == 0:\n",
    "        print(\"âš ï¸ No data pairs to visualize\")\n",
    "        return\n",
    "    \n",
    "    samples = random.sample(data_pairs, min(n_samples, len(data_pairs)))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, pair in enumerate(samples):\n",
    "        if idx >= 6:\n",
    "            break\n",
    "            \n",
    "        # Load image\n",
    "        img = Image.open(pair['image'])\n",
    "        \n",
    "        # Load text\n",
    "        with open(pair['text_file'], 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "        \n",
    "        # Display\n",
    "        axes[idx].imshow(img, cmap='gray')\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f\"Text: {text[:50]}...\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Handwritten Images with Ground Truth', \n",
    "                 fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "visualize_samples(data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "def analyze_dataset(data_pairs):\n",
    "    \"\"\"Analyze dataset characteristics.\"\"\"\n",
    "    if len(data_pairs) == 0:\n",
    "        print(\"âš ï¸ No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    text_lengths = []\n",
    "    image_sizes = []\n",
    "    \n",
    "    print(\"ðŸ“Š Analyzing dataset...\")\n",
    "    for pair in tqdm(data_pairs[:1000]):  # Analyze subset for speed\n",
    "        # Text length\n",
    "        with open(pair['text_file'], 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "            text_lengths.append(len(text))\n",
    "        \n",
    "        # Image size\n",
    "        img = Image.open(pair['image'])\n",
    "        image_sizes.append(img.size)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nðŸ“ Text Length Statistics:\")\n",
    "    print(f\"  Min: {min(text_lengths)} characters\")\n",
    "    print(f\"  Max: {max(text_lengths)} characters\")\n",
    "    print(f\"  Mean: {np.mean(text_lengths):.1f} characters\")\n",
    "    print(f\"  Median: {np.median(text_lengths):.1f} characters\")\n",
    "    \n",
    "    print(f\"\\nðŸ–¼ï¸ Image Size Statistics:\")\n",
    "    widths = [s[0] for s in image_sizes]\n",
    "    heights = [s[1] for s in image_sizes]\n",
    "    print(f\"  Width: {min(widths)} - {max(widths)} px (mean: {np.mean(widths):.0f})\")\n",
    "    print(f\"  Height: {min(heights)} - {max(heights)} px (mean: {np.mean(heights):.0f})\")\n",
    "    \n",
    "    # Visualize distributions\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    ax1.hist(text_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax1.set_xlabel('Text Length (characters)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Text Lengths')\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    ax2.scatter(widths, heights, alpha=0.3)\n",
    "    ax2.set_xlabel('Image Width (px)')\n",
    "    ax2.set_ylabel('Image Height (px)')\n",
    "    ax2.set_title('Image Dimensions')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_dataset(data_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Preprocessing Pipeline ðŸ”§\n",
    "\n",
    "Implement the preprocessing pipeline from our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    \"\"\"Preprocessing pipeline for handwriting images.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_size=(384, 384)):\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def preprocess(self, image: Image.Image) -> Image.Image:\n",
    "        \"\"\"Apply full preprocessing pipeline.\"\"\"\n",
    "        # Convert to numpy\n",
    "        img = np.array(image)\n",
    "        \n",
    "        # Convert to grayscale if needed\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Denoise\n",
    "        img = cv2.fastNlMeansDenoising(img, None, h=10)\n",
    "        \n",
    "        # Enhance contrast\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        img = clahe.apply(img)\n",
    "        \n",
    "        # Convert back to PIL\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        # Resize\n",
    "        img = img.resize(self.target_size, Image.LANCZOS)\n",
    "        \n",
    "        # Convert to RGB (TrOCR expects RGB)\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "        return img\n",
    "\n",
    "# Test preprocessing\n",
    "if len(data_pairs) > 0:\n",
    "    preprocessor = ImagePreprocessor()\n",
    "    \n",
    "    # Test on one image\n",
    "    test_img = Image.open(data_pairs[0]['image'])\n",
    "    processed_img = preprocessor.preprocess(test_img)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.imshow(test_img, cmap='gray')\n",
    "    ax1.set_title('Original')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(processed_img)\n",
    "    ax2.set_title('Preprocessed')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Preprocessing pipeline working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Dataset Preparation ðŸ“Š\n",
    "\n",
    "Create PyTorch dataset and dataloaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for handwriting OCR.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_pairs, processor, preprocessor=None, max_length=128):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.processor = processor\n",
    "        self.preprocessor = preprocessor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data_pairs[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(pair['image']).convert('RGB')\n",
    "        \n",
    "        # Preprocess if available\n",
    "        if self.preprocessor:\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "        \n",
    "        # Load text\n",
    "        with open(pair['text_file'], 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "        \n",
    "        # Process with TrOCR processor\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # Tokenize text\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        # Replace padding token id with -100 (ignored by loss)\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values.squeeze(),\n",
    "            \"labels\": labels.squeeze()\n",
    "        }\n",
    "\n",
    "print(\"âœ… Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "if len(data_pairs) > 0:\n",
    "    # Shuffle\n",
    "    random.shuffle(data_pairs)\n",
    "    \n",
    "    # Split 80/10/10\n",
    "    n_total = len(data_pairs)\n",
    "    n_train = int(0.8 * n_total)\n",
    "    n_val = int(0.1 * n_total)\n",
    "    \n",
    "    train_pairs = data_pairs[:n_train]\n",
    "    val_pairs = data_pairs[n_train:n_train+n_val]\n",
    "    test_pairs = data_pairs[n_train+n_val:]\n",
    "    \n",
    "    print(f\"ðŸ“Š Dataset splits:\")\n",
    "    print(f\"  Train: {len(train_pairs)} samples\")\n",
    "    print(f\"  Val:   {len(val_pairs)} samples\")\n",
    "    print(f\"  Test:  {len(test_pairs)} samples\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data pairs found - cannot split dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Training ðŸš€\n",
    "\n",
    "Fine-tune TrOCR model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TrOCR model and processor\n",
    "model_name = \"microsoft/trocr-base-handwritten\"\n",
    "\n",
    "print(f\"ðŸ“¥ Loading TrOCR model: {model_name}\")\n",
    "processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model loaded on {device}\")\n",
    "print(f\"ðŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "if len(data_pairs) > 0:\n",
    "    preprocessor = ImagePreprocessor()\n",
    "    \n",
    "    train_dataset = HandwritingDataset(train_pairs, processor, preprocessor)\n",
    "    val_dataset = HandwritingDataset(val_pairs, processor, preprocessor)\n",
    "    \n",
    "    print(f\"âœ… Datasets created\")\n",
    "    print(f\"  Train: {len(train_dataset)}\")\n",
    "    print(f\"  Val: {len(val_dataset)}\")\n",
    "    \n",
    "    # Test one batch\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"\\nðŸ“Š Sample batch shape:\")\n",
    "    print(f\"  Pixel values: {sample['pixel_values'].shape}\")\n",
    "    print(f\"  Labels: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./trocr-handwriting\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,  # Adjust based on time/resources\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    report_to=\"none\",  # Disable wandb\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "print(\"âœ… Training arguments configured\")\n",
    "print(f\"\\nðŸŽ¯ Training configuration:\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Mixed precision: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute metrics function\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute CER and WER metrics.\"\"\"\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate CER and WER\n",
    "    cer_score = cer(label_str, pred_str)\n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    \n",
    "    return {\n",
    "        \"cer\": cer_score,\n",
    "        \"wer\": wer_score\n",
    "    }\n",
    "\n",
    "print(\"âœ… Metrics function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "if len(data_pairs) > 0:\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Trainer created\")\n",
    "    print(\"\\nðŸš€ Ready to start training!\")\n",
    "    print(\"\\nNote: Training may take a while depending on dataset size and hardware.\")\n",
    "    print(\"      On Colab with GPU, expect ~1-2 hours for 3 epochs on medium dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "if len(data_pairs) > 0:\n",
    "    print(\"ðŸš€ Starting training...\\n\")\n",
    "    \n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\nâœ… Training completed!\")\n",
    "    print(f\"\\nðŸ“Š Training results:\")\n",
    "    print(f\"  Final loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"  Training time: {train_result.metrics['train_runtime']:.1f}s\")\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model(\"./trocr-handwriting-final\")\n",
    "    processor.save_pretrained(\"./trocr-handwriting-final\")\n",
    "    print(\"\\nðŸ’¾ Model saved to ./trocr-handwriting-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Evaluation ðŸ“ˆ\n",
    "\n",
    "Evaluate the model on test set and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "if len(data_pairs) > 0 and len(test_pairs) > 0:\n",
    "    test_dataset = HandwritingDataset(test_pairs, processor, preprocessor)\n",
    "    \n",
    "    print(\"ðŸ“Š Evaluating on test set...\\n\")\n",
    "    eval_results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    print(\"\\nâœ… Evaluation completed!\")\n",
    "    print(f\"\\nðŸ“Š Test Results:\")\n",
    "    print(f\"  Loss: {eval_results['eval_loss']:.4f}\")\n",
    "    print(f\"  CER: {eval_results['eval_cer']:.2%}\")\n",
    "    print(f\"  WER: {eval_results['eval_wer']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_text(image_path, model, processor, device):\n",
    "    \"\"\"Predict text from image.\"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    preprocessor = ImagePreprocessor()\n",
    "    image = preprocessor.preprocess(image)\n",
    "    \n",
    "    # Process\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values, max_length=128)\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"âœ… Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "if len(test_pairs) > 0:\n",
    "    n_samples = min(6, len(test_pairs))\n",
    "    samples = random.sample(test_pairs, n_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, pair in enumerate(samples):\n",
    "        # Load image\n",
    "        img = Image.open(pair['image'])\n",
    "        \n",
    "        # Load ground truth\n",
    "        with open(pair['text_file'], 'r', encoding='utf-8') as f:\n",
    "            ground_truth = f.read().strip()\n",
    "        \n",
    "        # Predict\n",
    "        prediction = predict_text(pair['image'], model, processor, device)\n",
    "        \n",
    "        # Calculate CER\n",
    "        cer_score = cer([ground_truth], [prediction])\n",
    "        \n",
    "        # Display\n",
    "        axes[idx].imshow(img, cmap='gray')\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        title = f\"Ground Truth: {ground_truth[:30]}...\\n\"\n",
    "        title += f\"Prediction: {prediction[:30]}...\\n\"\n",
    "        title += f\"CER: {cer_score:.2%}\"\n",
    "        \n",
    "        axes[idx].set_title(title, fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Model Predictions on Test Set', fontsize=14, y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Ensemble Implementation ðŸŽ¯\n",
    "\n",
    "Implement simple ensemble (for demo - can extend with Donut/LayoutLMv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnsemble:\n",
    "    \"\"\"Simple ensemble combining multiple predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, processor, device, n_runs=3):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.n_runs = n_runs\n",
    "    \n",
    "    def predict_ensemble(self, image_path):\n",
    "        \"\"\"Predict with ensemble (multiple runs with different seeds).\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(self.n_runs):\n",
    "            # Load and preprocess\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            preprocessor = ImagePreprocessor()\n",
    "            image = preprocessor.preprocess(image)\n",
    "            \n",
    "            # Process\n",
    "            pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "            \n",
    "            # Generate with different sampling\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    pixel_values,\n",
    "                    max_length=128,\n",
    "                    num_beams=5,\n",
    "                    do_sample=(i > 0),  # First run deterministic, others with sampling\n",
    "                    temperature=0.7 if i > 0 else 1.0\n",
    "                )\n",
    "            \n",
    "            # Decode\n",
    "            text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            predictions.append(text)\n",
    "        \n",
    "        # Simple voting: return most common prediction\n",
    "        from collections import Counter\n",
    "        counts = Counter(predictions)\n",
    "        best_prediction = counts.most_common(1)[0][0]\n",
    "        confidence = counts.most_common(1)[0][1] / self.n_runs\n",
    "        \n",
    "        return {\n",
    "            'prediction': best_prediction,\n",
    "            'confidence': confidence,\n",
    "            'all_predictions': predictions\n",
    "        }\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = SimpleEnsemble(model, processor, device, n_runs=3)\n",
    "print(\"âœ… Ensemble created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ensemble\n",
    "if len(test_pairs) > 0:\n",
    "    test_sample = test_pairs[0]\n",
    "    \n",
    "    print(\"ðŸ”„ Testing ensemble...\\n\")\n",
    "    result = ensemble.predict_ensemble(test_sample['image'])\n",
    "    \n",
    "    # Load ground truth\n",
    "    with open(test_sample['text_file'], 'r', encoding='utf-8') as f:\n",
    "        ground_truth = f.read().strip()\n",
    "    \n",
    "    print(f\"ðŸ“ Ground Truth: {ground_truth}\")\n",
    "    print(f\"\\nðŸŽ¯ Ensemble Prediction: {result['prediction']}\")\n",
    "    print(f\"ðŸ“Š Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"\\nðŸ” All Predictions:\")\n",
    "    for i, pred in enumerate(result['all_predictions'], 1):\n",
    "        print(f\"  Run {i}: {pred}\")\n",
    "    \n",
    "    # Display image\n",
    "    img = Image.open(test_sample['image'])\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Ensemble: {result['prediction']} (Conf: {result['confidence']:.2%})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Interactive Demo ðŸŽ®\n",
    "\n",
    "Try the model on custom images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and test your own image\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"ðŸ“¤ Upload a handwritten image to test:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if uploaded:\n",
    "        # Get first uploaded file\n",
    "        filename = list(uploaded.keys())[0]\n",
    "        \n",
    "        # Predict\n",
    "        print(f\"\\nðŸ” Processing {filename}...\\n\")\n",
    "        result = ensemble.predict_ensemble(filename)\n",
    "        \n",
    "        print(f\"ðŸŽ¯ Prediction: {result['prediction']}\")\n",
    "        print(f\"ðŸ“Š Confidence: {result['confidence']:.2%}\")\n",
    "        \n",
    "        # Display\n",
    "        img = Image.open(filename)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Prediction: {result['prediction']}\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ File upload only available in Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Export Model ðŸ’¾\n",
    "\n",
    "Save the trained model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "output_dir = \"./socar_trocr_final\"\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"âœ… Model saved to {output_dir}\")\n",
    "print(\"\\nðŸ“¦ Files saved:\")\n",
    "for file in Path(output_dir).glob('*'):\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model (Colab only)\n",
    "if IN_COLAB:\n",
    "    print(\"ðŸ“¦ Creating zip file for download...\")\n",
    "    !zip -r socar_trocr_model.zip {output_dir}\n",
    "    \n",
    "    from google.colab import files\n",
    "    files.download('socar_trocr_model.zip')\n",
    "    print(\"âœ… Model ready for download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary & Next Steps ðŸ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ðŸŽ‰ CONGRATULATIONS!\n",
    "\n",
    "You've successfully:\n",
    "âœ… Downloaded a real handwriting dataset\n",
    "âœ… Implemented preprocessing pipeline\n",
    "âœ… Fine-tuned TrOCR model\n",
    "âœ… Created an ensemble system\n",
    "âœ… Evaluated on test data\n",
    "âœ… Saved the trained model\n",
    "\n",
    "ðŸ“Š Final Results:\n",
    "  â€¢ CER: {:.2%}\n",
    "  â€¢ WER: {:.2%}\n",
    "  â€¢ Model ready for deployment!\n",
    "\n",
    "ðŸš€ Next Steps for SOCAR Hackathon:\n",
    "\n",
    "1. Fine-tune on SOCAR-specific data\n",
    "   - Collect internal handwriting samples\n",
    "   - Annotate with ground truth\n",
    "   - Re-run this notebook\n",
    "\n",
    "2. Add Donut model\n",
    "   - For form field extraction\n",
    "   - Follow similar training approach\n",
    "\n",
    "3. Implement LayoutLMv3\n",
    "   - For structured documents\n",
    "   - Requires bounding boxes\n",
    "\n",
    "4. Build production API\n",
    "   - Flask/FastAPI endpoint\n",
    "   - Docker container\n",
    "   - Deploy to server\n",
    "\n",
    "5. Create web demo\n",
    "   - Gradio interface (from repo)\n",
    "   - User-friendly UI\n",
    "\n",
    "ðŸ“š Resources:\n",
    "  â€¢ Full documentation: /docs folder\n",
    "  â€¢ Model guide: /docs/model_design.md\n",
    "  â€¢ Architecture: /docs/ARCHITECTURE.md\n",
    "\n",
    "Good luck at the hackathon! ðŸ†\n",
    "\"\"\".format(\n",
    "    eval_results.get('eval_cer', 0.0) if len(test_pairs) > 0 else 0.0,\n",
    "    eval_results.get('eval_wer', 0.0) if len(test_pairs) > 0 else 0.0\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
